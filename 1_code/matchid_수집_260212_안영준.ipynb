{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22c3e28-b0ac-4750-b44c-3140edf9ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0930c5b2-fac7-41d8-8160-b9c75e71edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kr_high = pd.read_csv('data/01_puuids/kr_puuids_high_tiers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773bc67f-ef4b-42e7-9374-8e5dfa24f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('data/02_match-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd223cf-ee5b-427b-b327-9a4a81277373",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''íŒ¨ì¹˜ ì¼ì'''\n",
    "kst = timezone(timedelta(hours=9))\n",
    "startTime_25_1 = datetime(2025, 1, 9, 7, 0, 0, tzinfo=kst)\n",
    "startTime_25_5 = datetime(2025, 3, 5, 3, 30, 0, tzinfo=kst)\n",
    "startTime_25_9 = datetime(2025, 4, 30, 7, 30, 0, tzinfo=kst)\n",
    "startTime_25_13 = datetime(2025, 6, 25, 7, 0, 0, tzinfo=kst)\n",
    "startTime_25_17 = datetime(2025, 8, 27, 8, 30, 0, tzinfo=kst)\n",
    "startTime_25_21 = datetime(2025, 10, 22, 7, 40, 0, tzinfo=kst)\n",
    "startTime_26_1 = datetime(2026, 1, 8, 7, 20, 0, tzinfo=kst)\n",
    "startTime_26_2 = datetime(2026, 2, 4, 6, 0, 0, tzinfo=kst)\n",
    "# print(int(startTime.timestamp()))\n",
    "\n",
    "# ì•¡íŠ¸ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„í•˜ê¸°\n",
    "patch_times = {\n",
    "    \"period_1\": (startTime_25_1, startTime_25_5), # 25_1 ~ 25_5 ì „\n",
    "    \"period_2\": (startTime_25_5, startTime_25_9), # 25_5 ~ 25_9 ì „\n",
    "    \"period_3\": (startTime_25_9, startTime_25_13), # 25_9 ~ 26_13 ì „\n",
    "    \"period_4\": (startTime_25_13, startTime_25_17), # 25_13 ~ 25_17 ì „\n",
    "    \"period_5\": (startTime_25_17, startTime_25_21), # 25_17 ~ 25_21 ì „\n",
    "    \"period_6\": (startTime_25_21, startTime_26_1), # 25_21 ~ 26_1 ì „\n",
    "    \"period_7\": (startTime_26_1, startTime_26_2) # 26_1 ~ 26_2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66ce723-a45a-435c-acd0-f9e896935947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# periods = []\n",
    "# for name, (start, end) in patch_times.items():\n",
    "#     periods.append({\n",
    "#         \"name\": name,\n",
    "#         \"start_ts\": int(start.timestamp()),\n",
    "#         \"end_ts\": int(end.timestamp()) if end else None\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f10afc1-d7c7-4791-9de9-0c7afec4f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_conv(unix_time):\n",
    "    dt = datetime.datetime.utcfromtimestamp(unix_time)\n",
    "    dt = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1237c-b324-449c-8ad5-ace2f870a7c2",
   "metadata": {},
   "source": [
    "- í•¨ìˆ˜ ëª¨ì€ê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821a2b4e-ba71-42e3-9f2f-688c1fc4a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"\"\n",
    "HEADERS = {\"X-Riot-Token\": API_KEY}\n",
    "\n",
    "REGION_MAPPING = {\n",
    "    'KR': 'asia', 'JP1': 'asia',\n",
    "    'BR1': 'americas', 'NA1': 'americas', 'LA1': 'americas', 'LA2': 'americas',\n",
    "    'EUN1': 'europe', 'EUW1': 'europe', 'TR1': 'europe', 'RU': 'europe', 'ME1': 'europe',\n",
    "    'OC1': 'sea', 'SG2': 'sea', 'TW2': 'sea', 'VN2': 'sea'\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"data/02_match-v5/collected_match_ids.csv\"\n",
    "PROGRESS_FILE = \"data/02_match-v5/processed_puuids.txt\"  # ìˆ˜ì§‘ ì™„ë£Œëœ ìœ ì € ì €ì¥ìš©\n",
    "SAVE_INTERVAL = 10\n",
    "\n",
    "# def get_matches_for_one_user(puuid, platform, startTime, total_count=100):\n",
    "#     routing = REGION_MAPPING.get(platform.upper(), 'asia')\n",
    "#     url = f\"https://{routing}.api.riotgames.com/lol/match/v5/matches/by-puuid/{puuid}/ids\"\n",
    "    \n",
    "#     user_match_ids = []\n",
    "#     for start_index in range(0, total_count, 100):\n",
    "#         params = {\n",
    "#             \"api_key\" : API_KEY,\n",
    "#             # \"queue\": 420,\n",
    "#             \"startTime\": startTime,\n",
    "#             # \"endTime\": endTime,\n",
    "#             \"start\": start_index, \n",
    "#             \"count\": 100}\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=HEADERS, params=params)\n",
    "#             if response.status_code == 200:\n",
    "#                 fetched_ids = response.json()\n",
    "#                 user_match_ids.extend(fetched_ids)\n",
    "#                 if len(fetched_ids) < 100: break # ë” ì´ìƒ ë°ì´í„° ì—†ìŒ\n",
    "#             elif response.status_code == 429:\n",
    "#                 print(\"!! Rate Limit! 10ì´ˆ ëŒ€ê¸°...\")\n",
    "#                 time.sleep(10)\n",
    "#                 continue # í˜„ì¬ êµ¬ê°„ ë‹¤ì‹œ ì‹œë„\n",
    "#             else:\n",
    "#                 break\n",
    "#             time.sleep(0.05) # Personal Key ì†ë„ ì¤€ìˆ˜\n",
    "#         except Exception as e:\n",
    "#             print(f\"ì˜¤ë¥˜: {e}\")\n",
    "#             break\n",
    "#     return user_match_ids\n",
    "\n",
    "# def save_to_csv(match_id_set, file_name):\n",
    "#     \"\"\"\n",
    "#     ìˆ˜ì§‘ëœ ë§¤ì¹˜ ID ì„¸íŠ¸ë¥¼ CSVì— ì €ì¥ (ì¤‘ë³µ ì œê±° í¬í•¨)\n",
    "#     \"\"\"\n",
    "#     new_df = pd.DataFrame(list(match_id_set), columns=['match_id'])\n",
    "    \n",
    "#     # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ê³ (header í¬í•¨), ìˆìœ¼ë©´ ì´ì–´ ì“°ê¸°(header ì œì™¸)\n",
    "#     if not os.path.exists(file_name):\n",
    "#         new_df.to_csv(file_name, index=False, mode='w', encoding='utf-8')\n",
    "#     else:\n",
    "#         new_df.to_csv(file_name, index=False, mode='a', header=False, encoding='utf-8')\n",
    "    \n",
    "#     print(f\"--- [ì¤‘ê°„ ì €ì¥ ì™„ë£Œ] {len(new_df)}ê°œì˜ ìƒˆë¡œìš´ ë§¤ì¹˜ ID ì €ì¥ë¨ ---\")\n",
    "\n",
    "# def collect_with_checkpoints(df_dict):\n",
    "#     current_session_matches = set() # í˜„ì¬ ì„¸ì…˜ì—ì„œì˜ ì¤‘ë³µ ë°©ì§€ìš©\n",
    "    \n",
    "#     for platform, df in df_dict.items():\n",
    "#         print(f\"\\nğŸš€ {platform} ì§€ì—­ ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "        \n",
    "#         for idx, puuid in enumerate(df['puuid']):\n",
    "#             m_ids = get_matches_for_one_user(puuid, platform)\n",
    "#             current_session_matches.update(m_ids)\n",
    "            \n",
    "#             # ì¤‘ê°„ ì €ì¥ ë¡œì§: SAVE_INTERVAL(10ëª…) ë§ˆë‹¤ ì‹¤í–‰\n",
    "#             if (idx + 1) % SAVE_INTERVAL == 0:\n",
    "#                 print(f\"[{idx+1}/{len(df)}] ì§„í–‰ ì¤‘...\", end=' ')\n",
    "#                 save_to_csv(current_session_matches, OUTPUT_FILE)\n",
    "#                 # ì €ì¥ í›„ ì„¸ì…˜ì„ ë¹„ì›Œì£¼ë©´ ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.\n",
    "#                 current_session_matches.clear() \n",
    "\n",
    "#         # ì§€ì—­ í•˜ë‚˜ê°€ ëë‚˜ë©´ ë‚¨ì€ ë°ì´í„° ì €ì¥\n",
    "#         if current_session_matches:\n",
    "#             save_to_csv(current_session_matches, OUTPUT_FILE)\n",
    "#             current_session_matches.clear()\n",
    "\n",
    "#     # ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²´ íŒŒì¼ì—ì„œ í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ì œê±°\n",
    "#     final_cleanup(OUTPUT_FILE)\n",
    "\n",
    "# def final_cleanup(file_name):\n",
    "#     print(\"\\nğŸ§¹ ì „ì²´ ë°ì´í„° ì¤‘ë³µ ì œê±° ì‘ì—… ì‹œì‘...\")\n",
    "#     df = pd.read_csv(file_name)\n",
    "#     before_count = len(df)\n",
    "#     df.drop_duplicates(subset=['match_id'], inplace=True)\n",
    "#     df.to_csv(file_name, index=False)\n",
    "#     print(f\"âœ… ìµœì¢… ì™„ë£Œ! ({before_count} -> {len(df)}ê°œ)\")\n",
    "\n",
    "# # ì‹¤í–‰\n",
    "# # collect_with_checkpoints(df_dict)\n",
    "\n",
    "# def load_processed_puuids():\n",
    "#     \"\"\"ì´ë¯¸ ìˆ˜ì§‘ ì™„ë£Œëœ ìœ ì € ëª©ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\"\"\"\n",
    "#     if os.path.exists(PROGRESS_FILE):\n",
    "#         with open(PROGRESS_FILE, 'r') as f:\n",
    "#             # í•œ ì¤„ì”© ì½ì–´ì„œ ì„¸íŠ¸ì— ì €ì¥ (ì¤‘ë³µ ì œê±° ë° ë¹ ë¥¸ ê²€ìƒ‰ìš©)\n",
    "#             return set(line.strip() for line in f)\n",
    "#     return set()\n",
    "\n",
    "# def save_progress(puuid_list):\n",
    "#     \"\"\"ìˆ˜ì§‘ ì™„ë£Œëœ ìœ ì € ëª©ë¡ì„ íŒŒì¼ì— ì¶”ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
    "#     # ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "#     os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)\n",
    "#     with open(PROGRESS_FILE, 'a') as f:\n",
    "#         for puuid in puuid_list:\n",
    "#             f.write(f\"{puuid}\\n\")\n",
    "\n",
    "# def collect_with_resume(df_dict, startTime):\n",
    "#     # 1. ì‘ì—… ì™„ë£Œ ëª…ë‹¨ ë¡œë“œ\n",
    "#     processed_puuids = load_processed_puuids()\n",
    "#     print(f\"ğŸ“Š ê¸°ì¡´ ì‘ì—… ê¸°ë¡ í™•ì¸: {len(processed_puuids)}ëª…ì˜ ìœ ì €ëŠ” ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "#     current_session_matches = set()\n",
    "#     newly_processed_this_session = [] # ì´ë²ˆ ì„¸ì…˜ì—ì„œ ìƒˆë¡œ ì™„ë£Œí•œ ìœ ì €ë“¤\n",
    "\n",
    "#     for platform, df in df_dict.items():\n",
    "#         print(f\"\\nğŸš€ {platform} ì§€ì—­ ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "        \n",
    "#         for idx, puuid in enumerate(df['puuid']):\n",
    "#             # 2. ê±´ë„ˆë›°ê¸° ë¡œì§\n",
    "#             if puuid in processed_puuids:\n",
    "#                 continue \n",
    "\n",
    "#             # 3. ë°ì´í„° ìˆ˜ì§‘\n",
    "#             m_ids = get_matches_for_one_user(puuid, platform, startTime, total_count=100)\n",
    "#             current_session_matches.update(m_ids)\n",
    "#             newly_processed_this_session.append(puuid)\n",
    "            \n",
    "#             # 4. ì¤‘ê°„ ì €ì¥ (SAVE_INTERVAL ë§ˆë‹¤)\n",
    "#             if len(newly_processed_this_session) >= SAVE_INTERVAL:\n",
    "#                 print(f\"[{idx+1}/{len(df)}] ì €ì¥ ì¤‘...\", end=' ')\n",
    "#                 save_to_csv(current_session_matches, OUTPUT_FILE)\n",
    "#                 save_progress(newly_processed_this_session) # ìœ ì € ëª…ë‹¨ë„ ì €ì¥\n",
    "                \n",
    "#                 # ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°\n",
    "#                 current_session_matches.clear()\n",
    "#                 processed_puuids.update(newly_processed_this_session)\n",
    "#                 newly_processed_this_session = []\n",
    "\n",
    "#         # ì§€ì—­ ì¢…ë£Œ í›„ ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬\n",
    "#         if newly_processed_this_session:\n",
    "#             save_to_csv(current_session_matches, OUTPUT_FILE)\n",
    "#             save_progress(newly_processed_this_session)\n",
    "#             current_session_matches.clear()\n",
    "#             newly_processed_this_session = []\n",
    "\n",
    "#     print(\"\\nâœ¨ ëª¨ë“  ì§€ì—­ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ==============================================================================================================\n",
    "# 260209 ìˆ˜ì •\n",
    "# 16ì£¼ ê°„ê²© ìˆ˜ì§‘\n",
    "# ê° ê¸°ê°„ ìœ ì €ë§ˆë‹¤ 100ê°œ matchid \n",
    "# ==============================================================================================================\n",
    "def get_matches_by_period(puuid, platform, start_dt, end_dt):\n",
    "    \"\"\"\n",
    "    start_dt, end_dt: datetime ê°ì²´ (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ timestampë¡œ ë³€í™˜)\n",
    "    \"\"\"\n",
    "    routing = REGION_MAPPING.get(platform.upper(), 'asia') # ê¸°ë³¸ê°’ asia ì„¤ì •\n",
    "    url = f\"https://{routing}.api.riotgames.com/lol/match/v5/matches/by-puuid/{puuid}/ids\"\n",
    "    \n",
    "    # datetime ê°ì²´ë¥¼ timestamp(int)ë¡œ ë³€í™˜\n",
    "    start_ts = int(start_dt.timestamp())\n",
    "    end_ts = int(end_dt.timestamp())\n",
    "\n",
    "    params = {\n",
    "        # \"api_key\": API_KEY, # í—¤ë”ì— ë„£ì—ˆìœ¼ë¯€ë¡œ íŒŒë¼ë¯¸í„°ì—ì„œëŠ” ì œì™¸í•´ë„ ë¨\n",
    "        \"type\": 'ranked',\n",
    "        \"startTime\": start_ts,\n",
    "        \"endTime\": end_ts,\n",
    "        \"count\": 100\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Rate Limit ë°©ì§€ ë”œë ˆì´ (í•„ìš”ì‹œ ì¡°ì ˆ)\n",
    "        time.sleep(1.2) \n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "            \n",
    "        elif response.status_code == 429:\n",
    "            # [ìˆ˜ì •ë¨] í—¤ë”ì—ì„œ Retry-After ê°’ì„ ê°€ì ¸ì™€ ëŒ€ê¸°\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 10)) # ì—†ìœ¼ë©´ ê¸°ë³¸ 10ì´ˆ\n",
    "            print(f\"âš ï¸ Rate Limit Exceeded (429). Sleeping for {retry_after} seconds...\")\n",
    "            time.sleep(retry_after + 1) # ì•ˆì „ì„ ìœ„í•´ 1ì´ˆ ë” ëŒ€ê¸°\n",
    "            return get_matches_by_period(puuid, platform, start_dt, end_dt) # ì¬ì‹œë„\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Request Exception: {e}\")\n",
    "        return []\n",
    "\n",
    "# ìˆ˜ì •ëœ ì €ì¥ í•¨ìˆ˜ (period ì •ë³´ í¬í•¨)\n",
    "def save_to_csv_with_period(match_data_list, file_name):\n",
    "    # ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    \n",
    "    new_df = pd.DataFrame(match_data_list, columns=['match_id', 'period'])\n",
    "    if not os.path.exists(file_name):\n",
    "        new_df.to_csv(file_name, index=False, mode='w', encoding='utf-8')\n",
    "    else:\n",
    "        new_df.to_csv(file_name, index=False, mode='a', header=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "def final_cleanup_by_period(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        return\n",
    "    df = pd.read_csv(file_name)\n",
    "    initial_len = len(df)\n",
    "    df.drop_duplicates(subset=['match_id'], inplace=True)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"âœ… ìµœì¢… ì •ë¦¬ ì™„ë£Œ. {initial_len} -> {len(df)} (ê³ ìœ  ë§¤ì¹˜ ìˆ˜)\")\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---\n",
    "\n",
    "def collect_by_patch_periods(df_dict):\n",
    "    processed_puuids = load_processed_puuids()\n",
    "    current_session_data = [] \n",
    "    newly_processed_puuids = []\n",
    "\n",
    "    for platform, df in df_dict.items():\n",
    "        print(f\"\\nğŸš€ {platform} ì§€ì—­ ìˆ˜ì§‘ ì‹œì‘... (ì´ {len(df)}ëª…)\")\n",
    "        \n",
    "        for idx, puuid in enumerate(df['puuid']):\n",
    "            if puuid in processed_puuids:\n",
    "                continue\n",
    "\n",
    "            # [ìˆ˜ì •ë¨] patch_times ë”•ì…”ë„ˆë¦¬ ìˆœíšŒ ë°©ì‹ ìˆ˜ì • (periods ë³€ìˆ˜ ì˜¤ë¥˜ í•´ê²°)\n",
    "            for period_name, (p_start, p_end) in patch_times.items():\n",
    "                m_ids = get_matches_by_period(puuid, platform, p_start, p_end)\n",
    "                \n",
    "                for m_id in m_ids:\n",
    "                    current_session_data.append((m_id, period_name))\n",
    "                \n",
    "            newly_processed_puuids.append(puuid)\n",
    "            \n",
    "            # ì¤‘ê°„ ì €ì¥ ë¡œì§\n",
    "            if len(newly_processed_puuids) >= SAVE_INTERVAL:\n",
    "                save_to_csv_with_period(current_session_data, OUTPUT_FILE)\n",
    "                save_progress(newly_processed_puuids)\n",
    "                \n",
    "                print(f\"[{idx+1}/{len(df)}] ì§„í–‰ ì¤‘.. ì €ì¥ ì™„ë£Œ (ì´ë²ˆ í„´ ë§¤ì¹˜: {len(current_session_data)}ê°œ)\")\n",
    "                \n",
    "                current_session_data = []\n",
    "                processed_puuids.update(newly_processed_puuids)\n",
    "                newly_processed_puuids = []\n",
    "\n",
    "    # ë‚¨ì€ ë°ì´í„° ì €ì¥\n",
    "    if newly_processed_puuids or current_session_data:\n",
    "        save_to_csv_with_period(current_session_data, OUTPUT_FILE)\n",
    "        save_progress(newly_processed_puuids)\n",
    "\n",
    "    # ìµœì¢… ì¤‘ë³µ ì œê±°\n",
    "    final_cleanup_by_period(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ad0a5-f419-461d-9b12-401a097d5326",
   "metadata": {},
   "source": [
    "## ìƒ˜í”Œ ìœ ì € ë°ì´í„° ì„œë²„ë§ˆë‹¤ í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2db93e-147b-46fa-a913-1a277ab4d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def perform_stratified_sampling(sample_fraction, input_file, output_file, min_per_group=1, stratify_col='tier', random_state=42):\n",
    "    \"\"\"\n",
    "    ê³„ì¸µì  ìƒ˜í”Œë§ ìˆ˜í–‰ (ê¸°ë³¸: í‹°ì–´ë³„ ìƒ˜í”Œë§).\n",
    "    - sample_fraction: ê° ê·¸ë£¹ì—ì„œ ë¹„ìœ¨ë¡œ ë½‘ì„ ë¹„ìœ¨\n",
    "    - min_per_group: ê·¸ë£¹ì´ ì‘ì„ ë•Œ ìµœì†Œ ë³´ì¥ ìƒ˜í”Œ ìˆ˜\n",
    "    - stratify_col: ê³„ì¸µí™”ì— ì‚¬ìš©í•  ì»¬ëŸ¼ ì´ë¦„\n",
    "    \"\"\"\n",
    "    print('ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...')\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    if stratify_col not in df.columns:\n",
    "        raise ValueError(f\"'{stratify_col}' ì»¬ëŸ¼ì´ ì…ë ¥ íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ê·¸ë£¹ë³„ë¡œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°\n",
    "    def _sample_group(g):\n",
    "        grp_size = len(g)\n",
    "        n = max(0, int(round(grp_size * sample_fraction)))\n",
    "        if n < min_per_group and grp_size > 0:\n",
    "            n = min(min_per_group, grp_size)\n",
    "        if n <= 0:\n",
    "            return g.iloc[0:0]\n",
    "        return g.sample(n=n, random_state=random_state)\n",
    "\n",
    "    sampled_df = df.groupby(stratify_col, group_keys=False).apply(_sample_group).reset_index(drop=True)\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥\n",
    "    print('\\nâœ… ìƒ˜í”Œë§ ì™„ë£Œ!')\n",
    "    print(f\"ì „ì²´ ë°ì´í„° ìˆ˜: {len(df):,} ëª…\")\n",
    "    print(f\"ìƒ˜í”Œë§ ë°ì´í„° ìˆ˜: {len(sampled_df):,} ëª…\")\n",
    "    print(f\"ìƒ˜í”Œ ë‚´ {stratify_col} ë¹„ì¤‘:\\n{(sampled_df[stratify_col].value_counts(normalize=True) * 100).round(2)}%\")\n",
    "\n",
    "    sampled_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"ğŸ’¾ ìƒ˜í”Œ ë°ì´í„°ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0a4de-a7cd-43f8-81b5-61a97fbcc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ì— ë§ê²Œ íŒŒì¼ ê²½ë¡œ ì…ë ¥\n",
    "file_paths = [\n",
    "    'data/01_puuids/LA2_puuids_high_tiers',\n",
    "    'data/01_puuids/LA2_puuids_iron_diamond',\n",
    "    'data/01_puuids/JP1_puuids_high_tiers',\n",
    "    'data/01_puuids/JP1_puuids_iron_diamond',\n",
    "    'data/01_puuids/ME1_puuids_high_tiers',\n",
    "    'data/01_puuids/ME1_puuids_iron_diamond',\n",
    "    'data/01_puuids/OC1_puuids_high_tiers',\n",
    "    'data/01_puuids/OC1_puuids_iron_diamond',\n",
    "    'data/01_puuids/RU_puuids_high_tiers',\n",
    "    'data/01_puuids/RU_puuids_iron_diamond',\n",
    "    'data/01_puuids/SG2_puuids_high_tiers',\n",
    "    'data/01_puuids/SG2_puuids_iron_diamond',\n",
    "    'data/01_puuids/TW2_puuids_high_tiers',\n",
    "    'data/01_puuids/TW2_puuids_iron_diamond',\n",
    "]\n",
    "\n",
    "for name in file_paths:\n",
    "    '''ìƒ˜í”Œë§ ë¹„ìœ¨ì€ ì¡°ì • í•„ìš”í•¨'''\n",
    "    sampling = 0.005\n",
    "    perform_stratified_sampling(\n",
    "        sample_fraction = sampling,\n",
    "        input_file=f'{name}.csv', \n",
    "        output_file=f'{name}_{sampling*100}%.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff50435-1e99-41cf-a7a7-3dd933f66fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œë§ ëœ ìœ ì €ë°ì´í„° ë¡œë“œ\n",
    "la2_high=pd.read_csv('data/01_puuids/LA2_puuids_high_tiers_0.5%.csv')\n",
    "sg2_high=pd.read_csv('data/01_puuids/SG2_puuids_high_tiers_0.5%.csv')\n",
    "jp1_high=pd.read_csv('data/01_puuids/JP1_puuids_high_tiers_0.5%.csv')\n",
    "tw2_high=pd.read_csv('data/01_puuids/TW2_puuids_high_tiers_0.5%.csv')\n",
    "ru_high=pd.read_csv('data/01_puuids/RU_puuids_high_tiers_0.5%.csv')\n",
    "oc1_high=pd.read_csv('data/01_puuids/OC1_puuids_high_tiers_0.5%.csv')\n",
    "me1_high=pd.read_csv('data/01_puuids/ME1_puuids_high_tiers_0.5%.csv')\n",
    "\n",
    "la2_low=pd.read_csv('data/01_puuids/LA2_puuids_iron_diamond_0.5%.csv')\n",
    "sg2_low=pd.read_csv('data/01_puuids/SG2_puuids_iron_diamond_0.5%.csv')\n",
    "jp1_low=pd.read_csv('data/01_puuids/JP1_puuids_iron_diamond_0.5%.csv')\n",
    "tw2_low=pd.read_csv('data/01_puuids/TW2_puuids_iron_diamond_0.5%.csv')\n",
    "ru_low=pd.read_csv('data/01_puuids/RU_puuids_iron_diamond_0.5%.csv')\n",
    "oc1_low=pd.read_csv('data/01_puuids/OC1_puuids_iron_diamond_0.5%.csv')\n",
    "me1_low=pd.read_csv('data/01_puuids/ME1_puuids_iron_diamond_0.5%.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450b96fd-5639-42df-a553-5227b3e49977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 7ê°œ ì§€ì—­ì˜ ë°ì´í„° í†µí•© ì™„ë£Œ!\n",
      "ëŒ€ìƒ ì§€ì—­: ['LA2', 'SG2', 'JP1', 'TW2', 'RU', 'OC1', 'ME1']\n"
     ]
    }
   ],
   "source": [
    "regions = [\"LA2\", \"SG2\", \"JP1\", \"TW2\", \"RU\", \"OC1\", \"ME1\"]\n",
    "\n",
    "low_df = [la2_low, sg2_low, jp1_low, tw2_low, ru_low, oc1_low, me1_low]\n",
    "high_df = [la2_high, sg2_high, jp1_high, tw2_high, ru_high, oc1_high, me1_high]\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©í•  ë”•ì…”ë„ˆë¦¬\n",
    "df_dict = {}\n",
    "\n",
    "# zip í™œìš©\n",
    "for region, low, high in zip(regions, low_df, high_df):\n",
    "    \n",
    "    # ì»¬ëŸ¼ ì‚­ì œ\n",
    "    cols_to_drop = ['division', 'leagueId']\n",
    "    low.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
    "    high.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
    "    \n",
    "    # Lowì™€ High í•©ì¹˜ê¸°\n",
    "    combined = pd.concat([low, high], ignore_index=True)\n",
    "    \n",
    "    # ì¤‘ë³µ ìœ ì €(puuid)ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í•œ ë²ˆ ë” ì œê±°\n",
    "    combined.drop_duplicates(subset=['puuid'], inplace=True)\n",
    "    \n",
    "    # ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ (Key: ì„œë²„ì´ë¦„, Value: í†µí•© ë°ì´í„°í”„ë ˆì„)\n",
    "    df_dict[region] = combined\n",
    "\n",
    "print(f\"âœ… {len(df_dict)}ê°œ ì§€ì—­ì˜ ë°ì´í„° í†µí•© ì™„ë£Œ!\")\n",
    "print(f\"ëŒ€ìƒ ì§€ì—­: {list(df_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d86911-b5b9-448e-80f5-1fac964448f9",
   "metadata": {},
   "source": [
    "## EXE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86c9c5-942c-4814-9035-56ab96bee619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collect_by_patch_periods(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e47f5-2834-4cd5-b92b-010be6e9a333",
   "metadata": {},
   "source": [
    "## ë§¤ì¹˜ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc7998a-6cc0-405c-9ac7-d62a58d712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchids = pd.read_csv('data/02_match-v5/collected_match_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e040a893-6587-49b4-8906-9413ecf75b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         match_id    period\n",
      "0  LA2_1501465065  period_2\n",
      "1  LA2_1501457348  period_2\n",
      "2  LA2_1501453622  period_2\n",
      "3  LA2_1501455030  period_2\n",
      "4  LA2_1501453109  period_2\n"
     ]
    }
   ],
   "source": [
    "print(matchids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ada28-b6dd-4ee0-80ee-1d11dcdaea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = f\"https://{routing}.api.riotgames.com/lol/match/v5/matches/{matchid}?api_key={API_KEY}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.14 (py3.14)",
   "language": "python",
   "name": "py3_14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
